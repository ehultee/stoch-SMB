{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d4edd3-138a-470e-ad72-87aeb9f2b501",
   "metadata": {},
   "source": [
    "Aggregate dEBM data to catchment means, based on the processing for GrSMBMIP.  \n",
    "\n",
    "- Phase 1: 2007-2017 catchment means + monthly lapse rates.  Restrict inflection point elevations to be consistent\n",
    "- Phase 2: 2010-2205 catchment means\n",
    "\n",
    "Last updated: 9 Dec 2022 | EHU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9b8235-9b73-4c5a-9881-783c809d9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import MultiPoint\n",
    "from shapely.ops import triangulate\n",
    "import shapefile\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyproj as pyproj\n",
    "from scipy import interpolate\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3abed1-ed35-4427-94f6-f1e402654c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "###------------------------\n",
    "### DATA READ-IN  AND PROJECTION\n",
    "###------------------------\n",
    "\n",
    "## Read in BedMachine grid to reproject SMB\n",
    "gl_bed_path ='/Volumes/GoogleDrive/My Drive/Greenland-data/BedMachine-Greenland/BedMachineGreenland-2017-09-20.nc'\n",
    "fh = Dataset(gl_bed_path, mode='r')\n",
    "xx = fh.variables['x'][:].copy() #x-coord (polar stereo (70, 45))\n",
    "yy = fh.variables['y'][:].copy() #y-coord\n",
    "ss = fh.variables['surface'][:].copy() # surface elevation\n",
    "M = fh.variables['mask'][:].copy() ## land cover type mask (2 = grounded ice)\n",
    "fh.close()\n",
    "\n",
    "## Read in Mouginot catchments from shapefile\n",
    "print('Reading in Mouginot catchments')\n",
    "catchment_fn = '/Volumes/GoogleDrive/My Drive/Greenland-data/Greenland-catchments-Mouginot/Greenland_Basins_PS_v1.4.2.'\n",
    "sf = shapefile.Reader(catchment_fn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb193e-4467-42bd-b8a0-4bb6151daa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in ISMIP6 grid for geometry\n",
    "print('Reading in ISMIP6 geometry')\n",
    "geom_path = '/Users/eultee@middlebury.edu/Documents/Research/data/5km_ISMIP6grid.nc'\n",
    "fh3 = Dataset(geom_path, mode='r')\n",
    "x_lon_debm = fh3.variables['lon'][:].copy() # longitude\n",
    "y_lat_debm = fh3.variables['lat'][:].copy() # latitude\n",
    "fh3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5094e6d9-46a9-44f4-b39b-472b858a90fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## dEBM read in for preliminary view\n",
    "print('Reading in example dEBM field')\n",
    "debm_path = '/Users/eultee@middlebury.edu/Documents/Research/data/dEBM-SMB-1850_2200/dEBM_SMB4Vincent_2007.nc'\n",
    "fh2 = Dataset(debm_path, mode='r')\n",
    "# x_debm = fh2.dimensions['x'].copy() #x-coord (polar stereo?)\n",
    "# y_debm = fh2.dimensions['y'].copy() #y-coord \n",
    "smb_debm = fh2.variables['SMB'][:].copy()\n",
    "fh2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb9df28-d196-437f-a82b-3652d0579dc8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(smb_debm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc908e3-4f64-4c6f-b9d1-a360c7033d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "###------------------------\n",
    "### SET UP SMB REPROJECTION\n",
    "###------------------------\n",
    "\n",
    "## Down-sample bed topo\n",
    "x_3km = xx[::20] # sample at ~3 km resolution\n",
    "y_3km = yy[::20]\n",
    "s_3km = ss[::20,::20] ## we'll use surface elevation for monthly lapse rate\n",
    "Xmat, Ymat = np.meshgrid(x_3km, y_3km) # Downsampled BedMachine coords\n",
    "\n",
    "print('Creating reprojected grid')\n",
    "wgs84 = pyproj.Proj(\"+init=EPSG:4326\") # LatLon with WGS84 datum used by SMB data\n",
    "psn_gl = pyproj.Proj(\"+init=epsg:3413\") # Polar Stereographic North used by BedMachine and Mankoff\n",
    "xs, ys = pyproj.transform(wgs84, psn_gl, x_lon_debm, y_lat_debm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc52067-644b-4e98-b09f-9001beb8df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###------------------------\n",
    "### CHOOSE CATCHMENTS\n",
    "###------------------------\n",
    "first_ctmt = 150\n",
    "last_ctmt = 260 ## confirmed that 260 should be end of the catchments available by running to 300, got error after 259\n",
    "catchments_to_pull = np.arange(first_ctmt, last_ctmt).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40495d0-ead0-4500-b1f3-05137d38f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "###------------------------\n",
    "### CREATE FRAMEWORK\n",
    "###------------------------\n",
    "\n",
    "caps_and_disjoint = []\n",
    "\n",
    "## Perform Delaunay triangulation over each catchment region\n",
    "tri_ctmts = {i: [] for i in catchments_to_pull}\n",
    "for i in catchments_to_pull:\n",
    "    print('Triangulating catchment {}'.format(sf.record(i)['NAME']))\n",
    "    c = MultiPoint(sf.shape(i).points)\n",
    "    tris = triangulate(c)\n",
    "    tri_ctmts[i] = tris\n",
    "    if 'ICE_CAPS' in sf.record(i)['NAME']:\n",
    "        caps_and_disjoint.append(i)\n",
    "    elif len(sf.shape(i).parts)>1:\n",
    "        caps_and_disjoint.append(i)\n",
    "\n",
    "\n",
    "years = range(2007,2018) ## calibration period 2007-2017 inclusive\n",
    "start_date = datetime.datetime(years[0],1,1)\n",
    "end_date = datetime.datetime(years[-1],12,31)\n",
    "dates = pd.date_range(start=start_date, end=end_date, freq='M')\n",
    "df_record = pd.DataFrame(columns=[i for i in catchments_to_pull], index=dates)\n",
    "df_elev_per_ctmt = {i: \n",
    "                    {n: \n",
    "                     {y: pd.DataFrame(columns=('elevation', \n",
    "                                                     'point_smb')) for y in years} for n in range(12)} for i in catchments_to_pull}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac71974-baaa-4f78-8307-a520d2d80910",
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_and_disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8374e16-0362-4e64-827c-4dd463444a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###------------------------\n",
    "### CATCHMENT-SUM FOR ALL MODELS\n",
    "###------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "for y in years:\n",
    "    ti = time.time()\n",
    "    fpath = '/Users/eultee@middlebury.edu/Documents/Research/data/dEBM-SMB-1850_2200/dEBM_SMB4Vincent_{}.nc'.format(y)\n",
    "    fh = Dataset(fpath, mode='r')\n",
    "    smb_m = fh.variables['SMB'][:].copy()\n",
    "    fh.close()\n",
    "    d_subset = [d for d in dates if d.year==y]\n",
    "    for i in range(len(smb_m)): # for each month\n",
    "        ## interpolate SMB\n",
    "        smb_ds = smb_m[i]\n",
    "        regridded_smb = interpolate.griddata((xs.ravel(), ys.ravel()), smb_ds.ravel(), (Xmat, Ymat), method='nearest')\n",
    "        ## Sample SMB at each Delaunay triangle and sum\n",
    "        for j in catchments_to_pull:\n",
    "            if j in caps_and_disjoint:\n",
    "#                 print('Identified catchment {} in ice caps and disjoint catchments. Skipping.'.format(j))\n",
    "#                 df_record.drop(columns=j, inplace=True)\n",
    "                pass\n",
    "            else:\n",
    "                catchment_sum = 0\n",
    "                area_sum = 0\n",
    "                triangles = tri_ctmts[j]\n",
    "                for tri in triangles:\n",
    "                    rep_x, rep_y = tri.representative_point().x, tri.representative_point().y\n",
    "                    area_m2 = tri.area\n",
    "                    smb_x = (np.abs(x_3km - rep_x)).argmin()\n",
    "                    smb_y = (np.abs(y_3km - rep_y)).argmin()\n",
    "                    nearest_smb = regridded_smb[smb_y, smb_x]\n",
    "                    if nearest_smb < -9990: ## catch fill values or unrealistic extreme values\n",
    "                        next_smb_x = np.argsort(np.abs(x_3km-rep_x))[1]\n",
    "                        next_smb_y = np.argsort(np.abs(y_3km-rep_y))[1]\n",
    "                        next_nearest_smb = regridded_smb[next_smb_y, next_smb_x]\n",
    "                        ### TODO: add an option for a fill value, if next nearest is also bad?\n",
    "                        local_val = next_nearest_smb*area_m2\n",
    "    #                 elif nearest_smb > 6000: ## HIRHAM and SNOWMODEL have positive fill val\n",
    "    #                     next_smb_x = np.argsort(np.abs(x_3km-rep_x))[1]\n",
    "    #                     next_smb_y = np.argsort(np.abs(y_3km-rep_y))[1]\n",
    "    #                     next_nearest_smb = regridded_smb[next_smb_y, next_smb_x]\n",
    "    #                     ### TODO: add an option for a fill value, if next nearest is also bad?\n",
    "    #                     local_val = next_nearest_smb*area_m2\n",
    "                    else:\n",
    "                        local_val = nearest_smb*area_m2\n",
    "                    catchment_sum += local_val\n",
    "                    area_sum += area_m2\n",
    "                df_record[j][d_subset[i]] = catchment_sum/area_sum\n",
    "    tf = time.time()\n",
    "    print('Finished processing year {} in time {}s'.format(y, tf-ti))\n",
    "t1 = time.time()\n",
    "print('Finished processing full period in time {}'.format(t1-t0))\n",
    "\n",
    "## Write to CSV\n",
    "csv_path = '/Users/eultee@middlebury.edu/Documents/Research/data/dEBM-processed/{}-catchments_{}to{}_mean-tseries.csv'.format(datetime.date.today().strftime('%Y%m%d'), first_ctmt, last_ctmt-1)\n",
    "df_record.to_csv(csv_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd442949-1805-4611-b9cd-9a823f688ea0",
   "metadata": {},
   "source": [
    "## Monthly SMB lapse rates per catchment\n",
    "Compute lapse rates, in the form of piecewise linear functions that may change by month.  `segments_fit` is copied from `stochSMB.py` in the main stoch-SMB repository.  It optimises for both BIC and AIC in locating inflection points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2131a90e-2f52-498b-97f6-583b12e9f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "def segments_fit(X, Y, maxcount):\n",
    "    xmin = X.min()\n",
    "    xmax = X.max()\n",
    "    \n",
    "    n = len(X)\n",
    "    \n",
    "    AIC_ = float('inf')\n",
    "    BIC_ = float('inf')\n",
    "    r_   = None\n",
    "    \n",
    "    for count in range(1, maxcount+1):\n",
    "        \n",
    "        seg = np.full(count - 1, (xmax - xmin) / count)\n",
    "\n",
    "        px_init = np.r_[np.r_[xmin, seg].cumsum(), xmax]\n",
    "        py_init = np.array([Y[np.abs(X - x) < (xmax - xmin) * 0.1].mean() for x in px_init])\n",
    "\n",
    "        def func(p):\n",
    "            seg = p[:count - 1]\n",
    "            py = p[count - 1:]\n",
    "            px = np.r_[np.r_[xmin, seg].cumsum(), xmax]\n",
    "            return px, py\n",
    "\n",
    "        def err(p): # This is RSS / n\n",
    "            px, py = func(p)\n",
    "            Y2 = np.interp(X, px, py)\n",
    "            return np.mean((Y - Y2)**2)\n",
    "\n",
    "        r = optimize.minimize(err, x0=np.r_[seg, py_init], method='Nelder-Mead')\n",
    "    \n",
    "        # Compute AIC/ BIC. \n",
    "        AIC = n * np.log10(err(r.x)) + 4 * count\n",
    "        BIC = n * np.log10(err(r.x)) + 2 * count * np.log(n)\n",
    "        \n",
    "        if((BIC < BIC_) & (AIC < AIC_)): # Continue adding complexity.\n",
    "            r_ = r\n",
    "            AIC_ = AIC\n",
    "            BIC_ = BIC\n",
    "        else: # Stop.\n",
    "            count = count - 1\n",
    "            break\n",
    "        \n",
    "    return func(r_.x) ## Return the last (n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801350e1-4a95-4ec3-9634-90d11f6ab114",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do another massive process per catchment...\n",
    " \n",
    "t0 = time.time()\n",
    "for y in years:\n",
    "    ti = time.time()\n",
    "    fpath = '/Users/eultee@middlebury.edu/Documents/Research/data/dEBM-SMB-1850_2200/dEBM_SMB4Vincent_{}.nc'.format(y)\n",
    "    fh = Dataset(fpath, mode='r')\n",
    "    smb_m = fh.variables['SMB'][:].copy()\n",
    "    fh.close()\n",
    "    d_subset = [d for d in dates if d.year==y]\n",
    "    for i in range(len(smb_m)): # for each month\n",
    "        ## interpolate SMB\n",
    "        smb_ds = smb_m[i]\n",
    "        regridded_smb = interpolate.griddata((xs.ravel(), ys.ravel()), smb_ds.ravel(), (Xmat, Ymat), method='nearest')\n",
    "        ## Sample SMB at each Delaunay triangle and write to dataframe\n",
    "        for j in catchments_to_pull:\n",
    "            if j in caps_and_disjoint:\n",
    "#                 print('Identified catchment {} in ice caps and disjoint catchments. Skipping.'.format(j))\n",
    "#                 df_record.drop(columns=j, inplace=True)\n",
    "                pass\n",
    "            else:\n",
    "                triangles = tri_ctmts[j]\n",
    "                elevations = []\n",
    "                smb_point_vals = []\n",
    "                for tri in triangles:\n",
    "                    rep_x, rep_y = tri.representative_point().x, tri.representative_point().y\n",
    "                    smb_x = (np.abs(x_3km - rep_x)).argmin()\n",
    "                    smb_y = (np.abs(y_3km - rep_y)).argmin()\n",
    "                    elevations.append(s_3km[smb_y, smb_x])\n",
    "                    nearest_smb = regridded_smb[smb_y, smb_x]\n",
    "                    if nearest_smb < -9990: ## catch fill values or unrealistic extreme values\n",
    "                        next_smb_x = np.argsort(np.abs(x_3km-rep_x))[1]\n",
    "                        next_smb_y = np.argsort(np.abs(y_3km-rep_y))[1]\n",
    "                        next_nearest_smb = regridded_smb[next_smb_y, next_smb_x]\n",
    "                        ### TODO: add an option for a fill value, if next nearest is also bad?\n",
    "                        smb_point_vals.append(next_nearest_smb)\n",
    "                    else:\n",
    "                        smb_point_vals.append(nearest_smb)\n",
    "                df_elev_per_ctmt[j][i][y] = df_elev_per_ctmt[j][i][y].assign(elevation=elevations,\n",
    "                                            point_smb=smb_point_vals)\n",
    "    tf = time.time()\n",
    "    print('Finished processing year {} in time {}s'.format(y, tf-ti))\n",
    "t1 = time.time()\n",
    "print('Finished processing full period in time {}'.format(t1-t0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e305354-2184-4b0a-a36a-0d545cdd9afd",
   "metadata": {},
   "source": [
    "Now stack all the same-month values from the full period and use these to regress the lapse rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac109c-cf8f-4112-a80d-5a0d7d2c2fd1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Test for just one catchment\n",
    "test_catchment = 1\n",
    "df_c = df_elev_per_ctmt[test_catchment]\n",
    "monthly_mbg = {i: [] for i in range(12)}\n",
    "\n",
    "for i in range(12):\n",
    "    d_to_join = [df_c[i][y] for y in years]\n",
    "    df = pd.concat(d_to_join) ## take just this month's values\n",
    "    pt_smbs = np.asarray(df.sort_values(by='elevation')['point_smb'])\n",
    "    anomalies = pt_smbs - np.mean(pt_smbs)\n",
    "    px, py = segments_fit(np.asarray(df.sort_values(by='elevation')['elevation']),\n",
    "                          anomalies,\n",
    "                          maxcount=2)\n",
    "    monthly_mbg[i] = (px,py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac04a06c-6ec2-4a90-821c-4469d4efb107",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up write-out dataframe and write for each catchment\n",
    "df_mbg_output = pd.DataFrame(columns=['Catchment {}'.format(i) for i in catchments_to_pull], index=np.arange(0,12))\n",
    "\n",
    "for c in catchments_to_pull:\n",
    "    if c in caps_and_disjoint:\n",
    "#                 print('Identified catchment {} in ice caps and disjoint catchments. Skipping.'.format(j))\n",
    "#                 df_record.drop(columns=j, inplace=True)\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            df_c = df_elev_per_ctmt[c]\n",
    "            monthly_mbg = {i: [] for i in range(12)}\n",
    "\n",
    "            for i in range(12):\n",
    "                d_to_join = [df_c[i][y] for y in years]\n",
    "                df = pd.concat(d_to_join) ## take just this month's values\n",
    "                pt_smbs = np.asarray(df.sort_values(by='elevation')['point_smb'])\n",
    "                anomalies = pt_smbs - np.mean(pt_smbs)\n",
    "                px, py = segments_fit(np.asarray(df.sort_values(by='elevation')['elevation']),\n",
    "                                      anomalies,\n",
    "                                      maxcount=2)\n",
    "                monthly_mbg[i] = (px,py)\n",
    "            df_mbg_output['Catchment {}'.format(c)] = pd.Series(monthly_mbg)\n",
    "        except AttributeError:\n",
    "            print('Error in catchment {}'.format(c))\n",
    "            df_mbg_output['Catchment {}'.format(c)] = pd.Series(np.nan, index=range(12))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e18ba06-29b9-49af-b987-5029635e10ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# ## figure out error in catchment 193, preventing write-out\n",
    "# fig, axs = plt.subplots(3, 4, sharex=True, sharey=True)\n",
    "# for i in range(12):\n",
    "#     ax = axs.ravel()[i]\n",
    "#     d_to_join = [df_elev_per_ctmt[193][i][y] for y in years]\n",
    "#     df_test = pd.concat(d_to_join)\n",
    "#     pt_smbs = np.asarray(df_test.sort_values(by='elevation')['point_smb'])\n",
    "#     anomalies = pt_smbs - np.mean(pt_smbs)\n",
    "#     ax.scatter(df_test.sort_values(by='elevation')['elevation'], anomalies, color=c(i%12), label=dates[i].strftime('%m/%Y'))\n",
    "#     ax.text(1000,-1100,'Month {}'.format(i+1))\n",
    "# plt.show()\n",
    "\n",
    "# df_test.sort_values(by='elevation')['elevation']\n",
    "anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d419c0f-1676-4191-b940-cd2a881361fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write to CSV\n",
    "csv_lr_path = '/Users/eultee@middlebury.edu/Documents/Research/data/dEBM-processed/{}-catchments_{}to{}-elev_SMB_lapserate.csv'.format(datetime.date.today().strftime('%Y%m%d'), first_ctmt, last_ctmt-1)\n",
    "df_mbg_output.to_csv(csv_lr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e397d4-c40a-420a-b002-bcc632eecde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mbg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a968d3a9-8ec2-414d-ac7b-1fbc7f98dd05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
