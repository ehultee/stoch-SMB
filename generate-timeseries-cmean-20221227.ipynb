{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42585de1-4cdc-402f-bb2a-32aee0cba67d",
   "metadata": {},
   "source": [
    "# Generating time series of surface mass balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6754d5-6e34-4402-9016-7bb6d4ec7f2d",
   "metadata": {},
   "source": [
    "Now that we have aggregated surface mass balance to catchments, explored the data, and identified some good candidate temporal models, we are ready to generate time series of SMB.  We will apply a selected temporal model and a spatially-informed noise method to generate a range of SMB time series with variability that matches that presented in process models.\n",
    "\n",
    "UPDATE 27-28 DEC 22: Read in catchment mean series rather than totals, and include all basins (range(0,260)) rather than only 1-200."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213d0be-9a4a-4235-b745-b557ae81d47b",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49b60b-2ef6-4283-8891-51e41666ef1f",
   "metadata": {},
   "source": [
    "Make sure that you have activated your `stisp` environment, defined for conda installation in the `environment.yml` file of this repository.  All of the packages we import here should be available within that environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f6265-cec2-4be2-b27a-9c4cb0a5d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import GraphicalLassoCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import scipy.linalg\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ee89a-5875-4beb-8a8d-f9bc382c9792",
   "metadata": {},
   "source": [
    "Adjust matplotlib default settings for plot label sizes and color scheme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7bf39-2fdf-4684-b10f-db53918cf791",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 16\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "noise_colors=cm.get_cmap('Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce79b338-24f1-43f4-ac5b-30131eeb12e3",
   "metadata": {},
   "source": [
    "We do all of our stochastic fitting to the output from individual process models, because fitting a multi-model mean would damp out some of the variability that we want to capture.  Data is aggregated to catchments defined by Mouginot & Rignot (link), tagged with ID numbers consistent with that dataset.  Here we select a process model to emulate and a glacier catchment to examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55245ed-b3aa-429a-a90a-d7022125fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['ANICE-ITM_Berends', 'CESM_kampenhout', 'dEBM_krebs','HIRHAM_mottram', \n",
    "                'NHM-SMAP_niwano', 'RACMO_noel', 'SNOWMODEL_liston']\n",
    "\n",
    "highlight_model = 'ANICE-ITM_Berends'\n",
    "highlight_catchment_name, highlight_catchment_id = 'KANGERLUSSUAQ', 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae0fb69-de4b-4885-a11e-224cbc374355",
   "metadata": {},
   "source": [
    "Now, we define some helper functions.  `read_catchment_series` will read in our aggregated data from CSV, `fit_catchment_series` will test several orders of AR(n) model and report which is the best fit to the data, and `find_AR_residuals` returns the residuals from a selected AR(n) model fit, which we will need for the spatial covariance method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb83fe-54fd-4a6c-849f-d1612e380962",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in time series\n",
    "def read_catchment_series(fpath, anomaly=True):\n",
    "    catchment_fpath = fpath\n",
    "    catchment_tseries = pd.read_csv(catchment_fpath, index_col=0, parse_dates=[0])\n",
    "    catchment_tseries.mask(catchment_tseries>1e30)\n",
    "    anomaly_series = catchment_tseries - catchment_tseries.mean()\n",
    "    if anomaly:\n",
    "        return anomaly_series\n",
    "    else:\n",
    "        return catchment_tseries\n",
    "\n",
    "def fit_catchment_series(tseries, which_model, comparison_n=range(1,6), \n",
    "                         seasonal=True):\n",
    "    bic_per_n = pd.DataFrame(index=comparison_n, columns=model_names)\n",
    "    \n",
    "    if 'multi' in which_model:  ## allow multi-model mode reporting\n",
    "        for m in model_names:\n",
    "            for n in comparison_n:\n",
    "                mod = AutoReg(tseries[m], n, trend='ct', seasonal=seasonal)\n",
    "                results = mod.fit()\n",
    "                bic_per_n[m][n] = results.bic\n",
    "            bic_per_n[m] = pd.to_numeric(bic_per_n[m])\n",
    "        best_n = bic_per_n.idxmin().mode()[0]\n",
    "    else:\n",
    "        for n in comparison_n:\n",
    "            mod = AutoReg(tseries[which_model], n, trend='ct', seasonal=seasonal)\n",
    "            results = mod.fit()\n",
    "            bic_per_n[which_model][n] = results.bic\n",
    "        bic_per_n[which_model] = pd.to_numeric(bic_per_n[which_model])\n",
    "        best_n = bic_per_n[which_model].idxmin()\n",
    "    \n",
    "    bic_difference = bic_per_n.transform(lambda x: x-x.min())\n",
    "    \n",
    "    return best_n, bic_difference\n",
    "\n",
    "def find_AR_residuals(tseries, which_model, chosen_n=1, \n",
    "                         seasonal=False):\n",
    "    mod = AutoReg(tseries[which_model], chosen_n, trend='ct', seasonal=seasonal)\n",
    "    results = mod.fit()\n",
    "    resids = results.resid\n",
    "    \n",
    "    return resids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da2fe98-4ca0-4b83-9bdd-2692b00ba6d2",
   "metadata": {},
   "source": [
    "## Time series from AR(n) fits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dcc215-a08a-4cd1-9f30-bbf131790cc7",
   "metadata": {},
   "source": [
    "First, we read in process-model-derived time series of SMB for the selected catchment, from all process models.  We resample the monthly data to annual sums with `pandas.Series.resample('A').sum()`.  \n",
    "\n",
    "We identify the best autoregressive model according to the most common best-fit among all process models, so that we can enforce some consistency later on.  You can easily change this by moving the fit into the `for` loop and setting `which_model=m` rather than `multi`.\n",
    "\n",
    "Finally, we fit an autoregressive model to each process model's output using `statsmodels.tsa.ar_model.AutoReg().fit()`, and we save the fitted AR models to a dictionary indexed by process-model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a6071-8ea9-42a1-a4b2-18565ee7008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b362cb-a8ad-46d9-abc8-b5925f7d4e09",
   "metadata": {},
   "source": [
    "We ignore warnings for now, because statsmodels is going to complain about parameter names.  This does not affect our results.  If you are debugging and want to see warnings, you can set `action=once` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2e1ecc-5fa3-4af3-b043-e8a38f63bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time series from AR(n) fits\n",
    "mod_fits = {m: [] for m in model_names}\n",
    "mods = {m: [] for m in model_names}\n",
    "\n",
    "ctmt_fpath = glob.glob('/Users/lizz/Documents/GitHub/Data_unsynced/SMBMIP-processed/*-catchment_{}_mean-tseries.csv'.format(highlight_catchment_id))[0]\n",
    "s = read_catchment_series(ctmt_fpath, anomaly=True)\n",
    "a = s.resample('A').sum()\n",
    "best_n, _ = fit_catchment_series(a, which_model='multi', seasonal=False)\n",
    "for m in model_names:\n",
    "    mod = AutoReg(a[m], best_n, trend='ct', seasonal=False).fit()\n",
    "    fv = mod.fittedvalues\n",
    "    r = mod.resid\n",
    "    mod_fits[m] = fv\n",
    "    mods[m] = mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f088728-5705-4847-9520-72057f88e2ef",
   "metadata": {},
   "source": [
    "Now we have our dictionary `mods` that stores AR models trained on process model output.  The elements of `mods` are `AutoRegResults` objects.  We can use the built-in method `predict` to generate several realizations of stochastic surface mass balance that approximate a given process model's series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd1997-4ade-4847-be3c-ef27214bcb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 12 Mar 2023\n",
    "## What happens if we insist on AR(1) for this example?\n",
    "best_n = 4\n",
    "for m in model_names:\n",
    "    mod = AutoReg(a[m], best_n, trend='ct', seasonal=False).fit()\n",
    "    fv = mod.fittedvalues\n",
    "    r = mod.resid\n",
    "    mod_fits[m] = fv\n",
    "    mods[m] = mod\n",
    "\n",
    "    ## Answer: the noise does an okay job, but the range in the projections is about 2x as large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99493f4-d4a4-4d37-b702-3435d4aff40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_realizations = 10\n",
    "preds = []\n",
    "for k in range(n_realizations):\n",
    "    mod = mods[highlight_model]\n",
    "    ar_smb_k = mod.predict(best_n, len(a)-1, dynamic=True)\n",
    "    preds.append(ar_smb_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac3459c-28cf-453f-934e-3e5991a63537",
   "metadata": {},
   "source": [
    "Note that we ask `predict` to use dynamic prediction (not using the input data values) after a short initial period.  You can experiment with different lengths of the initial period, but if you do not set the `dynamic` argument, the default behavior is that `predict` uses all available input values.  See the [statsmodels documentation](https://www.statsmodels.org/stable/generated/statsmodels.tsa.ar_model.AutoRegResults.predict.html#statsmodels.tsa.ar_model.AutoRegResults.predict) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a27be1f-1fd0-4513-aa59-f6388a2bec14",
   "metadata": {},
   "source": [
    "Let's plot the AR(n) realizations in `preds` and compare them with the SMB process model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d17fcd-bc6b-45ca-802b-9ddca8e5b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig1, ax1 = plt.subplots(figsize=(10,6))\n",
    "for k in range(n_realizations):\n",
    "    ax1.plot(preds[k], \n",
    "            color=noise_colors(k/n_realizations), alpha=0.5)\n",
    "ax1.plot(a[highlight_model], color='k',\n",
    "         label='{} output'.format(highlight_model))\n",
    "ax1.plot(np.NaN, np.NaN, color=noise_colors(0.5), label='AR({}) realizations'.format(best_n))\n",
    "ax1.set(xlabel='Year', ylabel=r'Catchment SMB anomaly [mm w.e. a$^{-1}$]',\n",
    "        xticks=(np.datetime64('1980-01-01'), np.datetime64('1990-01-01'),\n",
    "                np.datetime64('2000-01-01'), np.datetime64('2010-01-01')),\n",
    "        xticklabels=(1980,1990,2000,2010))\n",
    "ax1.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074318f9-d387-4134-ab28-fd9ed0e7168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: figure out generation of distinct AR(n) realizations...these should not all coincide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8ba28b-b9b4-41b7-ae46-4282d2116046",
   "metadata": {},
   "source": [
    "## Noise with spatial information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43896ebc-28b3-4ed1-a78a-66382f5704ff",
   "metadata": {},
   "source": [
    "Now, we will find a sparse correlation matrix for _all_ catchments, fit with the same order of AR model, to a single process model's SMB output.\n",
    "\n",
    "First we find and store the residuals of the AR fit to each catchment.  Then we find the empirical correlation matrix `emp_C`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbbb4c8-1056-4ab2-bf0b-a4c033f4b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_resids = []\n",
    "ts_toplot = []\n",
    "for i in range(0, 260):\n",
    "    # print(i)\n",
    "    ctmt_fpath = glob.glob('/Users/lizz/Documents/GitHub/Data_unsynced/SMBMIP-processed/*-catchment_{}_mean-tseries.csv'.format(i))[0]\n",
    "    s = read_catchment_series(ctmt_fpath, anomaly=True)\n",
    "    a = s.resample('A').sum()\n",
    "    ts_toplot.append(a)\n",
    "    r = find_AR_residuals(a, which_model=highlight_model, chosen_n=best_n, seasonal=False)\n",
    "    ar_resids.append(r)\n",
    "\n",
    "ar_resids -= np.mean(ar_resids, axis=0) # normalize\n",
    "emp_C = np.corrcoef(ar_resids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad0998-0846-46d3-a8af-05613e558121",
   "metadata": {},
   "source": [
    "For our case, where we have 30 years of data for 260 catchments, the empirical correlation matrix is unstable and contains too much information.  We use `sklearn.GraphicalLassoCV()` to find a sparse correlation matrix that approximates it and tamps down some of the unwanted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d6106-2a01-4375-a675-a9355e938eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.multivariate_normal(mean=np.zeros(len(ar_resids)), cov=emp_C, size=len(ar_resids[0]))\n",
    "\n",
    "gl_model = GraphicalLassoCV()\n",
    "gl_model.fit(X)\n",
    "cov_ = gl_model.covariance_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ed5e29-4e6e-49f6-aeb9-3c0b5067f432",
   "metadata": {},
   "source": [
    "Now, following Hu \\& Castruccio (2021 preprint), we generate noise series for each catchment using the sparse covariance matrix (in Cholesky lower-triangular decomposition) and the diagonal matrix of standard deviations of the individual catchments' AR residuals.  Here, the noise series will be the same length as `ar_resids`, representing however many years are in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016385a-a8c1-4b4e-8a1b-4ffa15505169",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = scipy.linalg.cholesky(cov_, lower=True)\n",
    "N = np.random.normal(size=np.shape(ar_resids)) # draws from normal dist\n",
    "\n",
    "D = np.diag(np.std(ar_resids,1)) ## diagonal matrix of standard devs\n",
    "# scaled_noise = D @ L @ N\n",
    "\n",
    "noise_realizations = []\n",
    "for j in range(n_realizations):\n",
    "    Nj = np.random.normal(size=np.shape(ar_resids))\n",
    "    noise_j = D @ L @ Nj\n",
    "    noise_realizations.append(noise_j[highlight_catchment_id-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e283e8-ac16-42d7-9a2b-022b9d913e9f",
   "metadata": {},
   "source": [
    "Plot the noise realizations alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73364007-24e8-47c4-9578-6005538a5615",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot noise and process model residual\n",
    "fig2, ax2 = plt.subplots(figsize=(10,6))\n",
    "for k in range(n_realizations):\n",
    "    ax2.plot(a.index[best_n::], noise_realizations[k], \n",
    "            color=noise_colors(k/n_realizations), alpha=0.5)\n",
    "ax2.plot(ts_toplot[highlight_catchment_id][highlight_model], color='k', ## previously used highlight_catchment_id-1 as index, but now we also read in catchment 0, so the -1 is not needed.\n",
    "         label='{} output'.format(highlight_model))\n",
    "ax2.plot(np.NaN, np.NaN, color=noise_colors(0.5), label='Stochastic realizations')\n",
    "ax2.set(xlabel='Year', ylabel=r'Residual SMB anomaly [mm w.e. a$^{-1}$]',\n",
    "        xticks=(np.datetime64('1980-01-01'), np.datetime64('1990-01-01'),\n",
    "                np.datetime64('2000-01-01'), np.datetime64('2010-01-01')),\n",
    "        xticklabels=(1980,1990,2000,2010))\n",
    "ax2.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d6e8b-a332-49c5-b2f3-a2d0d6aa2284",
   "metadata": {},
   "source": [
    "## A full stochastic model of SMB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e767fc-a296-4f44-bc2a-8b3476c11ebb",
   "metadata": {},
   "source": [
    "We can put together the AR(n) series and the spatially informed noise to generate \"full\" realizations of SMB.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3282a4-6476-47d3-b7cd-2e5ae383dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot a sum of the two\n",
    "fig3, ax3 = plt.subplots(figsize=(10,6))\n",
    "for k in range(n_realizations):\n",
    "    ax3.plot(mods[highlight_model].predict(best_n, len(a)-1, dynamic=best_n)\n",
    "             +noise_realizations[k], \n",
    "            color=noise_colors(k/n_realizations), alpha=0.5)\n",
    "ax3.plot(ts_toplot[highlight_catchment_id][highlight_model], color='k',\n",
    "         label='{} output'.format(highlight_model))\n",
    "ax3.plot(np.NaN, np.NaN, color=noise_colors(0.5), label='Stochastic realizations')\n",
    "ax3.set(xlabel='Year', ylabel=r'Catchment SMB anomaly [mm w.e. a$^{-1}$]',\n",
    "        xticks=(np.datetime64('1980-01-01'), np.datetime64('1990-01-01'),\n",
    "                np.datetime64('2000-01-01'), np.datetime64('2010-01-01')),\n",
    "        xticklabels=(1980,1990,2000,2010))\n",
    "ax3.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecbdaa8-2ea8-43b6-8bfb-b322b3ced378",
   "metadata": {},
   "source": [
    "We can even forecast SMB into the future with this combination.  Simply choose how many years of output should be created, feed this information to `predict`, and perform another matrix multiplication to generate noise series of the appropriate length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adfda6c-57ac-41da-bf33-4328ddb27452",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Future forecast\n",
    "yrs_after_1980 = 70\n",
    "noise_into_future = []\n",
    "preds_into_future = []\n",
    "for j in range(n_realizations):\n",
    "    ar_smb_k = mods[highlight_model].predict(best_n, yrs_after_1980, dynamic=True)\n",
    "    preds_into_future.append(ar_smb_k)\n",
    "    Nj = np.random.normal(size=(len(ar_resids), yrs_after_1980))\n",
    "    noise_j = D @ L @ Nj\n",
    "    noise_into_future.append(noise_j[highlight_catchment_id-1])\n",
    "\n",
    "fig4, ax4 = plt.subplots(figsize=(10,6))\n",
    "for k in range(n_realizations):\n",
    "    ax4.plot(preds_into_future[k]\n",
    "             +noise_into_future[k][best_n-1::], \n",
    "            color=noise_colors(k/n_realizations), lw=1.5, \n",
    "             alpha=0.8)\n",
    "ax4.plot(ts_toplot[highlight_catchment_id][highlight_model], \n",
    "         color='k', lw=2.0,\n",
    "         label='{} output'.format(highlight_model))\n",
    "ax4.plot(np.NaN, np.NaN, color=noise_colors(0.5), label='Stochastic realizations')\n",
    "ax4.set(xlabel='Year', ylabel=r'Catchment SMB anomaly [mm w.e. a$^{-1}$]')\n",
    "# ax4.set(xticks=(np.datetime64('1980-01-01'), np.datetime64('1990-01-01'),\n",
    "#                 np.datetime64('2000-01-01'), np.datetime64('2010-01-01'),\n",
    "#                 np.datetime64('2020-01-01'), np.datetime64('2030-01-01'),\n",
    "#                 np.datetime64('2040-01-01'), np.datetime64('2050-01-01'),),\n",
    "#         xticklabels=(1980,1990,2000,2010,2020,2030,2040,2050))\n",
    "ax4.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d3ac0b-3686-4962-8305-62acfe234fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
