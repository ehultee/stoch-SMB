{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "028c6e62-0e61-4f0b-b057-d1277deba6cd",
   "metadata": {},
   "source": [
    "## Generating series scaled to local grid by elevation\n",
    "First, do everything from generate-timeseries.ipynb (and catchment-full_series_generation.py).\n",
    "\n",
    "Edit 12 Mar 2023: Use updated catchment means, force AR(4) for Kangerlussuaq, fewer realizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b284519d-d4f1-461f-a4ae-e524f9e6e4b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Generate example time series of SMB for a given catchment\n",
    "Pre-select AR(1) for annual variability portion\n",
    "Include noise from sparse covar method\n",
    "\n",
    "Created on Mon Jun 21 11:54:52 2021\n",
    "\n",
    "@author: lizz\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.covariance import GraphicalLassoCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import scipy.linalg\n",
    "import glob\n",
    "\n",
    "\n",
    "model_names = ['ANICE-ITM_Berends', 'CESM_kampenhout', 'dEBM_krebs','HIRHAM_mottram', \n",
    "                'NHM-SMAP_niwano', 'RACMO_noel', 'SNOWMODEL_liston']\n",
    "highlight_model = 'ANICE-ITM_Berends'\n",
    "highlight_catchment_name, highlight_catchment_id = 'KANGERLUSSUAQ', 101\n",
    "\n",
    "## Read in time series\n",
    "def read_catchment_series(fpath, anomaly=True):\n",
    "    catchment_fpath = fpath\n",
    "    catchment_tseries = pd.read_csv(catchment_fpath, index_col=0, parse_dates=[0])\n",
    "    catchment_tseries.mask(catchment_tseries>1e30)\n",
    "    anomaly_series = catchment_tseries - catchment_tseries.mean()\n",
    "    if anomaly:\n",
    "        return anomaly_series\n",
    "    else:\n",
    "        return catchment_tseries\n",
    "\n",
    "def fit_catchment_series(tseries, which_model, comparison_n=range(1,6), \n",
    "                         seasonal=True):\n",
    "    bic_per_n = pd.DataFrame(index=comparison_n, columns=model_names)\n",
    "    \n",
    "    if 'multi' in which_model:  ## allow multi-model mode reporting\n",
    "        for m in model_names:\n",
    "            for n in comparison_n:\n",
    "                mod = AutoReg(tseries[m], n, trend='ct', seasonal=seasonal)\n",
    "                results = mod.fit()\n",
    "                bic_per_n[m][n] = results.bic\n",
    "            bic_per_n[m] = pd.to_numeric(bic_per_n[m])\n",
    "        best_n = bic_per_n.idxmin().mode()[0]\n",
    "    else:\n",
    "        for n in comparison_n:\n",
    "            mod = AutoReg(tseries[which_model], n, trend='ct', seasonal=seasonal)\n",
    "            results = mod.fit()\n",
    "            bic_per_n[which_model][n] = results.bic\n",
    "        bic_per_n[which_model] = pd.to_numeric(bic_per_n[which_model])\n",
    "        best_n = bic_per_n[which_model].idxmin()\n",
    "    \n",
    "    bic_difference = bic_per_n.transform(lambda x: x-x.min())\n",
    "    \n",
    "    return best_n, bic_difference\n",
    "\n",
    "def find_AR_residuals(tseries, which_model, chosen_n=1, \n",
    "                         seasonal=False):\n",
    "    mod = AutoReg(tseries[which_model], chosen_n, trend='ct', seasonal=seasonal)\n",
    "    results = mod.fit()\n",
    "    resids = results.resid\n",
    "    \n",
    "    return resids\n",
    "\n",
    "## Time series from AR(n) fits\n",
    "mod_fits = {m: [] for m in model_names}\n",
    "mods = {m: [] for m in model_names}\n",
    "\n",
    "ctmt_fpath = glob.glob('/Users/lizz/Documents/GitHub/Data_unsynced/SMBMIP-processed/*-catchment_{}-tseries.csv'.format(highlight_catchment_id))[0]\n",
    "s = read_catchment_series(ctmt_fpath, anomaly=True)\n",
    "a = s.resample('A').sum()\n",
    "best_n, _ = fit_catchment_series(a, which_model='multi', seasonal=False)\n",
    "for m in model_names:\n",
    "    mod = AutoReg(a[m], best_n, trend='ct', seasonal=False).fit()\n",
    "    fv = mod.fittedvalues\n",
    "    r = mod.resid\n",
    "    mod_fits[m] = fv\n",
    "    mods[m] = mod\n",
    "\n",
    "\n",
    "## Residuals with spatial information\n",
    "ar_resids = []\n",
    "ts_toplot = []\n",
    "for i in range(1, 200):\n",
    "    # print(i)\n",
    "    ctmt_fpath = glob.glob('/Users/lizz/Documents/GitHub/Data_unsynced/SMBMIP-processed/*-catchment_{}-tseries.csv'.format(i))[0]\n",
    "    s = read_catchment_series(ctmt_fpath, anomaly=True)\n",
    "    a = s.resample('A').sum()\n",
    "    ts_toplot.append(a)\n",
    "    r = find_AR_residuals(a, which_model=highlight_model, chosen_n=best_n, seasonal=False)\n",
    "    ar_resids.append(r)\n",
    "\n",
    "ar_resids -= np.mean(ar_resids, axis=0) # normalize\n",
    "emp_C = np.corrcoef(ar_resids)\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.multivariate_normal(mean=np.zeros(len(ar_resids)), cov=emp_C, size=len(ar_resids[0]))\n",
    "\n",
    "gl_model = GraphicalLassoCV()\n",
    "gl_model.fit(X)\n",
    "cov_ = gl_model.covariance_\n",
    "\n",
    "## Now take the Cholesky decomposition of the sparse cov matrix and generate noise with it\n",
    "L = scipy.linalg.cholesky(cov_, lower=True)\n",
    "N = np.random.normal(size=np.shape(ar_resids)) # draws from normal dist\n",
    "\n",
    "D = np.diag(np.std(ar_resids,1)) ## diagonal matrix of standard devs\n",
    "scaled_noise = D @ L @ N\n",
    "\n",
    "\n",
    "## Plot example time series of resids\n",
    "## set matplotlib font size defaults\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 16\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "noise_realizations = []\n",
    "## Several realizations\n",
    "for j in range(100):\n",
    "    Nj = np.random.normal(size=np.shape(ar_resids))\n",
    "    noise_j = D @ L @ Nj\n",
    "    noise_realizations.append(noise_j[highlight_catchment_id-1])\n",
    "noise_colors=cm.get_cmap('Blues')\n",
    "\n",
    "## Plot a sum of the two\n",
    "fig2, ax2 = plt.subplots(figsize=(10,6))\n",
    "for k in range(len(noise_realizations)):\n",
    "    ax2.plot(mods[highlight_model].predict(best_n, len(a)-1, dynamic=best_n)\n",
    "             +noise_realizations[k], \n",
    "            color=noise_colors(k/len(noise_realizations)), alpha=0.5)\n",
    "ax2.plot(ts_toplot[highlight_catchment_id-1][highlight_model], color='k',\n",
    "         label='{} output'.format(highlight_model))\n",
    "ax2.plot(np.NaN, np.NaN, color=noise_colors(0.5), label='Stochastic realizations')\n",
    "ax2.set(xlabel='Year', ylabel=r'Catchment SMB anomaly [mm w.e. a$^{-1}$]',\n",
    "        xticks=(np.datetime64('1980-01-01'), np.datetime64('1990-01-01'),\n",
    "                np.datetime64('2000-01-01'), np.datetime64('2010-01-01')),\n",
    "        xticklabels=(1980,1990,2000,2010))\n",
    "ax2.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "## Future forecast\n",
    "yrs_after_1980 = 70\n",
    "noise_into_future = []\n",
    "for j in range(100):\n",
    "    Nj = np.random.normal(size=(len(ar_resids), yrs_after_1980))\n",
    "    noise_j = D @ L @ Nj\n",
    "    noise_into_future.append(noise_j[highlight_catchment_id-1])\n",
    "\n",
    "fig3, ax3 = plt.subplots(figsize=(10,6))\n",
    "for k in range(len(noise_into_future)):\n",
    "    ax3.plot(mods[highlight_model].predict(best_n, yrs_after_1980, dynamic=2*best_n)\n",
    "             +noise_into_future[k][best_n-1::], \n",
    "            color=noise_colors(k/len(noise_realizations)), alpha=0.5)\n",
    "ax3.plot(ts_toplot[highlight_catchment_id-1][highlight_model], color='k',\n",
    "         label='{} output'.format(highlight_model))\n",
    "ax3.plot(np.NaN, np.NaN, color=noise_colors(0.5), label='Stochastic realizations')\n",
    "# ax2.set(xlabel='Year', ylabel=r'Catchment SMB anomaly [mm w.e. a$^{-1}$]',\n",
    "#         xticks=(np.datetime64('1980-01-01'), np.datetime64('1990-01-01'),\n",
    "#                 np.datetime64('2000-01-01'), np.datetime64('2010-01-01')),\n",
    "#         xticklabels=(1980,1990,2000,2010))\n",
    "ax3.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe9b1d7-c3f3-4171-9ed5-80a1792d5a9b",
   "metadata": {},
   "source": [
    "## Map plot\n",
    "Read in Kangerlussuaq catchment outline, compute catchment mean, extract point values, and compute anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc89bd-1a17-42a8-966c-b2c12c7fb209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import MultiPoint, Polygon, Point\n",
    "from shapely.ops import triangulate\n",
    "from scipy.spatial import distance\n",
    "import shapefile\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyproj as pyproj\n",
    "from scipy import interpolate\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe644840-9338-4ab9-a7f8-d2bdf82cb945",
   "metadata": {},
   "outputs": [],
   "source": [
    "###------------------------\n",
    "### CHOOSE CATCHMENTS\n",
    "###------------------------\n",
    "catchments_to_pull = (101,)\n",
    "\n",
    "###------------------------\n",
    "### DATA READ-IN  AND PROJECTION\n",
    "###------------------------\n",
    "\n",
    "## Read in BedMachine grid to reproject SMB\n",
    "gl_bed_path ='/Users/lizz/Documents/GitHub/Data_unsynced/BedMachine-Greenland/BedMachineGreenland-2017-09-20.nc'\n",
    "fh = Dataset(gl_bed_path, mode='r')\n",
    "xx = fh.variables['x'][:].copy() # x-coord (polar stereo (70, 45))\n",
    "yy = fh.variables['y'][:].copy() # y-coord\n",
    "ss = fh.variables['surface'][:].copy() # surface elevation\n",
    "fh.close()\n",
    "\n",
    "## Read in Mouginot catchments from shapefile\n",
    "print('Reading in Mouginot catchments')\n",
    "catchment_fn = '/Users/lizz/Documents/GitHub/Data_unsynced/Greenland-catchments-Mouginot/Greenland_Basins_PS_v1.4.2.'\n",
    "sf = shapefile.Reader(catchment_fn) \n",
    "\n",
    "## Example SMB field read in for grid\n",
    "print('Reading in example SMB field')\n",
    "# nhm_smb_path = '/Users/lizz/Documents/GitHub/Data_unsynced/SMBMIP/NHM-SMAP_niwano-monthly-ERA-Interim-1980.nc'\n",
    "nhm_smb_path = '/Volumes/Backup Plus/Large data moved 20220331/SMBMIP/NHM-SMAP_niwano-monthly-ERA-Interim-1980.nc'\n",
    "fh2 = Dataset(nhm_smb_path, mode='r')\n",
    "xlon_nhm = fh2.variables['LON'][:].copy() #x-coord (latlon)\n",
    "ylat_nhm = fh2.variables['LAT'][:].copy() #y-coord (latlon)\n",
    "fh2.close()\n",
    "\n",
    "###------------------------\n",
    "### SET UP SMB REPROJECTION\n",
    "###------------------------\n",
    "\n",
    "## Down-sample bed topo\n",
    "x_3km = xx[::20] # sample at ~3 km resolution\n",
    "y_3km = yy[::20]\n",
    "s_3km = ss[::20,::20]\n",
    "\n",
    "## Down-sample SMB\n",
    "x_lon_h = xlon_nhm[::10, ::10] \n",
    "y_lat_h = ylat_nhm[::10, ::10] # resolution about 10 km\n",
    "\n",
    "print('Creating reprojected meshgrid')\n",
    "wgs84 = pyproj.Proj(\"+init=EPSG:4326\") # LatLon with WGS84 datum used by SMB data\n",
    "psn_gl = pyproj.Proj(\"+init=epsg:3413\") # Polar Stereographic North used by BedMachine and Mankoff\n",
    "xs, ys = pyproj.transform(wgs84, psn_gl, x_lon_h, y_lat_h)\n",
    "Xmat, Ymat = np.meshgrid(x_3km, y_3km) # Downsampled BedMachine coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd3fc15-3ef2-42b9-ac93-989cc4949cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------\n",
    "## CREATE FRAMEWORK\n",
    "##------------------------\n",
    "\n",
    "## Identify grid points within catchment\n",
    "pts_all = [(xs.ravel()[k], ys.ravel()[k]) for k in range(len(xs.ravel()))]\n",
    "pt_ctmts = {i: [] for i in catchments_to_pull}\n",
    "for i in catchments_to_pull:\n",
    "    print('Point-checking catchment {}'.format(sf.record(i)['NAME']))\n",
    "    c = Polygon(sf.shape(i).points)\n",
    "    pts_in = [Point(p).within(c) for p in pts_all]\n",
    "    pts = np.asarray(pts_all)[pts_in]\n",
    "    pt_ctmts[i] = pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52649b23-9a7f-4868-8fb1-cfdd86c6dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_for_rescale = c.area ## in projection coordinates, which should be sq m\n",
    "\n",
    "rescaled_projections = {i: [] for i in range(len(noise_realizations))}\n",
    "for k in range(len(noise_realizations)):\n",
    "    pred = mods[highlight_model].predict(best_n, len(a)-1, dynamic=best_n)+noise_realizations[k]\n",
    "    rp = np.array(pred)/area_for_rescale\n",
    "    rescaled_projections[k] = rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7707e04-e2e1-48fb-8e3c-2a92bcd257e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_projections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c018023-9349-4489-bb45-a66e72708e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rp = pd.DataFrame(rescaled_projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf92b6a-5f66-411a-9910-324347cf1a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88840c5-ebf0-4c74-a4e1-9c3ad0053186",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create data frames to store per-model data\n",
    "# model_names = ['ANICE-ITM_Berends', 'CESM_kampenhout', 'dEBM_krebs','HIRHAM_mottram', \n",
    "#                 'NHM-SMAP_niwano', 'RACMO_noel', 'SNOWMODEL_liston']\n",
    "model_names = ['ANICE-ITM_Berends',]\n",
    "years = range(1980,2010)\n",
    "start_date = datetime.datetime(years[0],1,1)\n",
    "end_date = datetime.datetime(years[-1],12,31)\n",
    "dates = pd.date_range(start=start_date, end=end_date, freq='M')\n",
    "example_df_per_ctchmnt = {i: \n",
    "                  {m: \n",
    "                   {n: \n",
    "                    {y: pd.DataFrame(columns=('elevation', \n",
    "                                                'point_smb'))  for y in years}\n",
    "                    for n in range(12)}\n",
    "                     for m in model_names}\n",
    "                      for i in catchments_to_pull}\n",
    "\n",
    "###------------------------\n",
    "### EXTRACT EXAMPLE SMB FIELD FOR CATCHMENT\n",
    "###------------------------\n",
    "\n",
    "## Store regridded SMBs for use in each catchment to take all available data\n",
    "smb_ex_monthly = {i: [] for i in range(12)}\n",
    "\n",
    "for m in ('ANICE-ITM_Berends',):\n",
    "    t0 = time.time()\n",
    "    if m=='CESM_kampenhout':\n",
    "        vname = 'SMBCORR'\n",
    "    else:\n",
    "        vname = 'SMBcorr'\n",
    "    for y in years:\n",
    "        ti = time.time()\n",
    "#         fpath = '/Users/lizz/Documents/GitHub/Data_unsynced/SMBMIP/{}-monthly-ERA-Interim-{}.nc'.format(m, y)\n",
    "        fpath = '/Volumes/Backup Plus/Large data moved 20220331/SMBMIP/{}-monthly-ERA-Interim-{}.nc'.format(m, y)\n",
    "        fh = Dataset(fpath, mode='r')\n",
    "        smb_m = fh.variables[vname][:].copy()\n",
    "        fh.close()\n",
    "        d_subset = [d for d in dates if d.year==y]\n",
    "        for i in range(len(smb_m)): # for each month\n",
    "            ## downsample SMB\n",
    "            smb_ds = smb_m[i][::10, ::10]\n",
    "            ## Sample SMB at each Delaunay triangle and sum\n",
    "            for j in catchments_to_pull:\n",
    "                points = pt_ctmts[j]\n",
    "                elevations = []\n",
    "                smb_point_vals = smb_ds.ravel()[pts_in] ## go back and revise this to use new pts_in for every catchment\n",
    "                for p in points:\n",
    "                    surf_x = (np.abs(x_3km - p[0])).argmin()\n",
    "                    surf_y = (np.abs(y_3km - p[1])).argmin()\n",
    "                    elevations.append(s_3km[surf_y, surf_x])\n",
    "                example_df_per_ctchmnt[j][m][i][y] = example_df_per_ctchmnt[j][m][i][y].assign(elevation=elevations,\n",
    "                                            point_smb=smb_point_vals)\n",
    "                \n",
    "        tf = time.time()\n",
    "        print('Finished processing year {} in time {}s'.format(y, tf-ti))\n",
    "    t1 = time.time()\n",
    "    print('Finished processing model {} in time {}'.format(m, t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2ef5dd-5d18-4e44-bf89-2176f45b8132",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select points for tseries comparison\n",
    "series_points = [\n",
    "    (497.73, -2298.93), # near terminus of Kangerlussuaq, in km Easting, Northing\n",
    "    (397.96, -2342.95), # small positive anomaly, southwest\n",
    "    (483.06, -2161.00) # farther up in accumulation area\n",
    "    ]\n",
    "ser = {'Point 1':[], 'Point 2':[], 'Point 3':[]}\n",
    "elev = []\n",
    "for i,p in enumerate(series_points):\n",
    "    idx = distance.cdist([1000*np.array(p)], pt_ctmts[101]).argmin()\n",
    "    ts = []\n",
    "    elev.append(example_df_per_ctchmnt[101]['ANICE-ITM_Berends'][1][1980]['elevation'][idx])\n",
    "    for y in years:\n",
    "        for m in range(12):\n",
    "            d = example_df_per_ctchmnt[101]['ANICE-ITM_Berends'][m][y]['point_smb'][idx]\n",
    "            ts.append(d)\n",
    "    ser['Point {}'.format(i+1)] = ts\n",
    "r = pd.date_range(start='1980', end='2010', freq='M') ## dates for plotting\n",
    "df = pd.DataFrame(ser, index=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a6ffce-53f5-46d4-9b2f-0a36c22e1a02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## plot the series\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2)\n",
    "df.plot(ax=ax1)\n",
    "ax1.set(ylabel='SMB anomaly [mm w.e.]', xlabel='Time')\n",
    "df_rp.plot(ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266db1c4-5acf-40f6-8d58-a5fcc930b374",
   "metadata": {},
   "source": [
    "## Scale by elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4092fb1a-e95b-475c-8811-41a643b13e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "elev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee750e18-5c98-4ca6-982c-76d8bed5309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stochSMB import segments_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0114d1-8572-4c42-923a-77e69f9f3baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = example_df_per_ctchmnt[101]['ANICE-ITM_Berends']\n",
    "dict_per_month = {i: [] for i in range(12)}\n",
    "for i in range(12):\n",
    "    d_to_join = [df_c[i][y] for y in years]\n",
    "    dict_per_month[i] = pd.concat(d_to_join)\n",
    "\n",
    "monthly_mbg = {i: [] for i in range(12)}\n",
    "monthly_mult = {i: [] for i in range(12)}\n",
    "for i in range(12):\n",
    "    df_m = dict_per_month[i]\n",
    "    pt_smbs = np.asarray(df_m.sort_values(by='elevation')['point_smb'])\n",
    "    anomalies = pt_smbs - np.mean(pt_smbs)\n",
    "    mult_factor = pt_smbs/np.mean(pt_smbs)\n",
    "    px, py = segments_fit(np.asarray(df_m.sort_values(by='elevation')['elevation']),\n",
    "                          # np.asarray(df.sort_values(by='elevation')['point_smb']),\n",
    "                          anomalies,\n",
    "                          maxcount=2)\n",
    "    pxm, pym = segments_fit(np.asarray(df_m.sort_values(by='elevation')['elevation']),\n",
    "                            # np.asarray(df.sort_values(by='elevation')['point_smb']),\n",
    "                            mult_factor,\n",
    "                            maxcount=2)\n",
    "    monthly_mbg[i] = interpolate.interp1d(px,py)\n",
    "    monthly_mult[i] = interpolate.interp1d(pxm, pym)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9f2dba-1257-48d0-a0cd-c5f906a5fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_correction = {'Point 1': [], 'Point 2':[], 'Point 3': []}\n",
    "monthly_mult_correction = {'Point 1': [], 'Point 2':[], 'Point 3': []}\n",
    "for j, el in enumerate(elev):\n",
    "    mc = [monthly_mbg[i](el) for i in range(12)]\n",
    "    mult_mc = [monthly_mult[i](el) for i in range(12)]\n",
    "    monthly_correction['Point {}'.format(j+1)] = mc\n",
    "    monthly_mult_correction['Point {}'.format(j+1)] = mult_mc\n",
    "df_mc = pd.DataFrame(monthly_correction)\n",
    "df_mc_mult = pd.DataFrame(monthly_mult_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65cb520-a8b8-41ff-a750-2a60fa1eb0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c33232-a6a0-4936-9f86-ab9c186d48be",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85107992-0ca7-4b0d-845a-f8dc2f480ffb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "monthly_preds_p1 = {r[j]: [] for j in range(len(df))} ## create one row for each date\n",
    "monthly_preds_p2 = {r[j]: [] for j in range(len(df))} ## create one row for each date\n",
    "monthly_preds_p3 = {r[j]: [] for j in range(len(df))} ## create one row for each date\n",
    "\n",
    "\n",
    "count = 0\n",
    "for k in range(len(df)):\n",
    "    scale_factor_1 = df_mc['Point 1'][k%12]\n",
    "    scale_factor_2 = df_mc['Point 2'][k%12]\n",
    "    scale_factor_3 = df_mc['Point 3'][k%12]\n",
    "    monthly_preds_p1[r[k]] = scale_factor_1 + np.array(df_rp.iloc[count])\n",
    "    monthly_preds_p2[r[k]] = scale_factor_2 + np.array(df_rp.iloc[count])\n",
    "    monthly_preds_p3[r[k]] = scale_factor_3 + np.array(df_rp.iloc[count])\n",
    "    if k%12==11: # if we've reached the end of a year\n",
    "        count+=1\n",
    "    if count>28:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c0e8f-e90c-4033-a2bf-719ba2291ee7",
   "metadata": {},
   "source": [
    "Now let's try this again, using multiplicative factor instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b412e2-ca44-4c41-bdbb-cc374f0bbf70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## No, let's not.  It doesn't work well.\n",
    "\n",
    "# monthly_preds_p1 = {r[j]: [] for j in range(len(df))} ## create one row for each date\n",
    "# monthly_preds_p2 = {r[j]: [] for j in range(len(df))} ## create one row for each date\n",
    "# monthly_preds_p3 = {r[j]: [] for j in range(len(df))} ## create one row for each date\n",
    "\n",
    "\n",
    "# count = 0\n",
    "# for k in range(len(df)):\n",
    "#     scale_factor_1 = df_mc_mult['Point 1'][k%12]\n",
    "#     scale_factor_2 = df_mc_mult['Point 2'][k%12]\n",
    "#     scale_factor_3 = df_mc_mult['Point 3'][k%12]\n",
    "#     monthly_preds_p1[r[k]] = scale_factor_1 * np.array(df_rp.iloc[count])\n",
    "#     monthly_preds_p2[r[k]] = scale_factor_2 * np.array(df_rp.iloc[count])\n",
    "#     monthly_preds_p3[r[k]] = scale_factor_3 * np.array(df_rp.iloc[count])\n",
    "#     if k%12==11: # if we've reached the end of a year\n",
    "#         count+=1\n",
    "#     if count>28:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8450b1-336d-4c81-8073-fa598191fe63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mpp1 = pd.DataFrame.from_dict(monthly_preds_p1, orient='index')\n",
    "df_mpp2 = pd.DataFrame.from_dict(monthly_preds_p2, orient='index')\n",
    "df_mpp3 = pd.DataFrame.from_dict(monthly_preds_p3, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb1d0fb-bbbd-424f-ad1b-9239afda2612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3, sharex=True)\n",
    "for k in range(len(df_mpp1.columns)): ## plot all the realizations\n",
    "    ax1.plot(df_mpp1[k], \n",
    "           color=noise_colors(k/len(df_mpp1.columns)), alpha=0.5) \n",
    "ax1.plot(df['Point 1'], color='k', lw=2.0) #plot the true local SMB from the process model\n",
    "ax1.plot(np.NaN, np.NaN, color=noise_colors(0.5), label='Stochastic realizations')\n",
    "ax1.set(#xlabel='Year',\n",
    "        xticks=(np.datetime64('1980-01-01'), np.datetime64('1990-01-01'),\n",
    "                np.datetime64('2000-01-01'), np.datetime64('2010-01-01')),\n",
    "        xticklabels=(1980,1990,2000,2010))\n",
    "ax1.legend(loc='best')\n",
    "\n",
    "for k in range(len(df_mpp2.columns)): ## plot all the realizations\n",
    "    ax2.plot(df_mpp2[k], \n",
    "           color=noise_colors(k/len(df_mpp2.columns)), alpha=0.5) \n",
    "ax2.plot(df['Point 2'], color='k', lw=2.0) #plot the true local SMB from the process model\n",
    "ax2.plot(np.NaN, np.NaN, color=noise_colors(0.5), label='Stochastic realizations')\n",
    "ax2.set(#xlabel='Year', \n",
    "        ylabel=r'Local SMB [mm w.e. a$^{-1}$]',\n",
    "        xticks=(np.datetime64('1980-01-01'), np.datetime64('1990-01-01'),\n",
    "                np.datetime64('2000-01-01'), np.datetime64('2010-01-01')),\n",
    "        xticklabels=(1980,1990,2000,2010))\n",
    "ax2.legend(loc='best')\n",
    "\n",
    "for k in range(len(df_mpp3.columns)): ## plot all the realizations\n",
    "    ax3.plot(df_mpp3[k], \n",
    "           color=noise_colors(k/len(df_mpp3.columns)), alpha=0.5) \n",
    "ax3.plot(df['Point 3'], color='k', lw=2.0) #plot the true local SMB from the process model\n",
    "ax3.plot(np.NaN, np.NaN, color=noise_colors(0.5), label='Stochastic realizations')\n",
    "ax3.set(xlabel='Year',\n",
    "        xticks=(np.datetime64('1980-01-01'), np.datetime64('1990-01-01'),\n",
    "                np.datetime64('2000-01-01'), np.datetime64('2010-01-01')),\n",
    "        xticklabels=(1980,1990,2000,2010))\n",
    "ax3.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a69aa-6497-4efa-883d-a4040f2c1713",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for k in range(len(df_mpp3.columns)): ## plot all the realizations\n",
    "    ax.plot(df_mpp3[k], \n",
    "           color=noise_colors(k/len(df_mpp3.columns)), alpha=0.5) \n",
    "ax.plot(df['Point 3'], color='k', lw=2.0) #plot the true local SMB from the process model\n",
    "ax.plot(np.NaN, np.NaN, color=noise_colors(0.5), label='Stochastic realizations')\n",
    "ax.set(xlabel='Year',\n",
    "        xticks=(np.datetime64('1980-01-01'), np.datetime64('1990-01-01'),\n",
    "                np.datetime64('2000-01-01'), np.datetime64('2010-01-01')),\n",
    "        xticklabels=(1980,1990,2000,2010))\n",
    "ax.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df_rp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4f475-22e7-4b0d-86d3-868dc71ae059",
   "metadata": {},
   "source": [
    "## Revision 16 Nov 2022: plot only two points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66a566-26c5-455d-b51a-c28d618b3822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, sharex=True, sharey=True)\n",
    "for k in range(len(df_mpp1.columns)): ## plot all the realizations\n",
    "    ax1.plot(df_mpp1[k], \n",
    "           color=noise_colors(k/len(df_mpp1.columns)), alpha=0.5) \n",
    "ax1.plot(df['Point 1'], color='k', lw=2.0) #plot the true local SMB from the process model\n",
    "ax1.plot(np.NaN, np.NaN, color=noise_colors(0.5), label='Stochastic realizations')\n",
    "ax1.set(#xlabel='Year',\n",
    "        xticks=(np.datetime64('1980-01-01'), np.datetime64('1990-01-01'),\n",
    "                np.datetime64('2000-01-01'), np.datetime64('2010-01-01')),\n",
    "        xticklabels=(1980,1990,2000,2010))\n",
    "# ax1.legend(loc='best')\n",
    "\n",
    "for k in range(len(df_mpp2.columns)): ## plot all the realizations\n",
    "    ax2.plot(df_mpp3[k], \n",
    "           color=noise_colors(k/len(df_mpp3.columns)), alpha=0.5) \n",
    "ax2.plot(df['Point 3'], color='k', lw=2.0, label='Process model output') #plot the true local SMB from the process model\n",
    "ax2.plot(np.NaN, np.NaN, color=noise_colors(0.5), label='Stochastic realizations')\n",
    "ax2.set(xlabel='Year', \n",
    "#         ylabel=r'Local SMB [mm w.e. a$^{-1}$]',\n",
    "        xticks=(np.datetime64('1980-01-01'), np.datetime64('1990-01-01'),\n",
    "                np.datetime64('2000-01-01'), np.datetime64('2010-01-01')),\n",
    "        xticklabels=(1980,1990,2000,2010))\n",
    "ax2.legend(loc='best')\n",
    "\n",
    "# for k in range(len(df_mpp3.columns)): ## plot all the realizations\n",
    "#     ax3.plot(df_mpp3[k], \n",
    "#            color=noise_colors(k/len(df_mpp3.columns)), alpha=0.5) \n",
    "# ax3.plot(df['Point 3'], color='k', lw=2.0) #plot the true local SMB from the process model\n",
    "# ax3.plot(np.NaN, np.NaN, color=noise_colors(0.5), label='Stochastic realizations')\n",
    "# ax3.set(xlabel='Year',\n",
    "#         xticks=(np.datetime64('1980-01-01'), np.datetime64('1990-01-01'),\n",
    "#                 np.datetime64('2000-01-01'), np.datetime64('2010-01-01')),\n",
    "#         xticklabels=(1980,1990,2000,2010))\n",
    "# ax3.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2711bb5-b081-4319-9990-b20e97d946cc",
   "metadata": {},
   "source": [
    "## Add map of where these points are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2974a5c4-c1be-4721-a185-18f0cf48fca9",
   "metadata": {},
   "source": [
    "Single panel map with points highlighted by stars.  Colors on map could add more relevant info - how about the seasonal swing?  This would highlight how sub-annual variability is dominated by the near-terminus region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3007056-6a52-45a9-9587-3a12312ef0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_points = np.asarray([\n",
    "    (497.73, -2298.93), # near terminus of Kangerlussuaq, in km Easting, Northing\n",
    "#     (397.96, -2342.95), # small positive anomaly, southwest\n",
    "    (483.06, -2161.00) # farther up in accumulation area\n",
    "    ])\n",
    "\n",
    "seasonal_swing = np.abs(example_df_per_ctchmnt[101]['ANICE-ITM_Berends'][11][1980]['point_smb'] - example_df_per_ctchmnt[101]['ANICE-ITM_Berends'][6][1980]['point_smb'])\n",
    "\n",
    "fig, ax = plt.subplots(1, sharex=True, sharey=True)\n",
    "\n",
    "example_smb = example_df_per_ctchmnt[101]['ANICE-ITM_Berends'][11][1980]['point_smb']\n",
    "sc = ax.scatter(0.001*pt_ctmts[101][:,0], 0.001*pt_ctmts[101][:,1], \n",
    "            c=seasonal_swing,\n",
    "                vmin = 0, vmax = 1000,\n",
    "           cmap='Oranges')\n",
    "ax.scatter(highlight_points[:,0], highlight_points[:,1], marker='*', s=10,\n",
    "          color='b')\n",
    "ax.set(aspect=1,\n",
    "      xticks=(300, 400, 500), yticks=(-2100, -2200, -2300),\n",
    "      )\n",
    "cb = fig.colorbar(sc, ax=ax, extend='max', ticks=(0, 500, 1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1582fe7-92e8-4a2d-ab9b-d15aa2c3f0b7",
   "metadata": {},
   "source": [
    "Now flip the plotting order of the series so that they appear roughly corresponding with their map location (terminus lower on the page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850142f2-9266-4d3e-ada8-0a5b99f689d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, sharex=True, sharey=True)\n",
    "for k in range(len(df_mpp2.columns)): ## plot all the realizations\n",
    "    ax1.plot(df_mpp3[k], \n",
    "           color=noise_colors(k/len(df_mpp3.columns)), alpha=0.5) \n",
    "ax1.plot(df['Point 3'], color='k', lw=2.0, label='Process model output') #plot the true local SMB from the process model\n",
    "ax1.plot(np.NaN, np.NaN, color=noise_colors(0.5), label='Stochastic realizations')\n",
    "ax1.set(xlabel='Year', \n",
    "#         ylabel=r'Local SMB [mm w.e. a$^{-1}$]',\n",
    "        xticks=(np.datetime64('1980-01-01'), np.datetime64('1990-01-01'),\n",
    "                np.datetime64('2000-01-01'), np.datetime64('2010-01-01')),\n",
    "        xticklabels=(1980,1990,2000,2010))\n",
    "ax1.legend(loc='best')\n",
    "\n",
    "\n",
    "for k in range(len(df_mpp1.columns)): ## plot all the realizations\n",
    "    ax2.plot(df_mpp1[k], \n",
    "           color=noise_colors(k/len(df_mpp1.columns)), alpha=0.5) \n",
    "ax2.plot(df['Point 1'], color='k', lw=2.0) #plot the true local SMB from the process model\n",
    "ax2.plot(np.NaN, np.NaN, color=noise_colors(0.5), label='Stochastic realizations')\n",
    "ax2.set(#xlabel='Year',\n",
    "        xticks=(np.datetime64('1980-01-01'), np.datetime64('1990-01-01'),\n",
    "                np.datetime64('2000-01-01'), np.datetime64('2010-01-01')),\n",
    "        xticklabels=(1980,1990,2000,2010))\n",
    "# ax2.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20371cd-a4f4-4741-8e45-482f3010c970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
