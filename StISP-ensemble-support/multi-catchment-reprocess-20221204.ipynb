{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d4edd3-138a-470e-ad72-87aeb9f2b501",
   "metadata": {},
   "source": [
    "Re-run the multi-catchment processing scheme to aggregate data to catchment means.  TODO: load in new dEBM data and process calibration period and forward projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9b8235-9b73-4c5a-9881-783c809d9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import MultiPoint\n",
    "from shapely.ops import triangulate\n",
    "import shapefile\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyproj as pyproj\n",
    "from scipy import interpolate\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6dc390-0359-4dae-a4db-073de454e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###------------------------\n",
    "### DATA READ-IN  AND PROJECTION\n",
    "###------------------------\n",
    "\n",
    "## Read in BedMachine grid to reproject SMB\n",
    "gl_bed_path ='/Users/lizz/Documents/GitHub/Data_unsynced/BedMachine-Greenland/BedMachineGreenland-2017-09-20.nc'\n",
    "fh = Dataset(gl_bed_path, mode='r')\n",
    "xx = fh.variables['x'][:].copy() #x-coord (polar stereo (70, 45))\n",
    "yy = fh.variables['y'][:].copy() #y-coord\n",
    "M = fh.variables['mask'][:].copy() ## land cover type mask (2 = grounded ice)\n",
    "fh.close()\n",
    "\n",
    "## Read in Mouginot catchments from shapefile\n",
    "print('Reading in Mouginot catchments')\n",
    "catchment_fn = '/Users/lizz/Documents/GitHub/Data_unsynced/Greenland-catchments-Mouginot/Greenland_Basins_PS_v1.4.2.'\n",
    "sf = shapefile.Reader(catchment_fn) \n",
    "\n",
    "## Example SMB field read in for grid\n",
    "print('Reading in example SMB field')\n",
    "nhm_smb_path = '/Volumes/GoogleDrive/My Drive/SMBMIP/NHM-SMAP_niwano-monthly-ERA-Interim-1980.nc'\n",
    "fh2 = Dataset(nhm_smb_path, mode='r')\n",
    "xlon_nhm = fh2.variables['LON'][:].copy() #x-coord (latlon)\n",
    "ylat_nhm = fh2.variables['LAT'][:].copy() #y-coord (latlon)\n",
    "fh2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc908e3-4f64-4c6f-b9d1-a360c7033d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "###------------------------\n",
    "### SET UP SMB REPROJECTION\n",
    "###------------------------\n",
    "\n",
    "## Down-sample bed topo\n",
    "x_3km = xx[::20] # sample at ~3 km resolution\n",
    "y_3km = yy[::20]\n",
    "Xmat, Ymat = np.meshgrid(x_3km, y_3km) # Downsampled BedMachine coords\n",
    "\n",
    "## Down-sample SMB\n",
    "x_lon_h = xlon_nhm[::2, ::2] \n",
    "y_lat_h = ylat_nhm[::2, ::2] # resolution about 2 km\n",
    "\n",
    "print('Creating reprojected grid')\n",
    "wgs84 = pyproj.Proj(\"+init=EPSG:4326\") # LatLon with WGS84 datum used by SMB data\n",
    "psn_gl = pyproj.Proj(\"+init=epsg:3413\") # Polar Stereographic North used by BedMachine and Mankoff\n",
    "xs, ys = pyproj.transform(wgs84, psn_gl, x_lon_h, y_lat_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc52067-644b-4e98-b09f-9001beb8df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###------------------------\n",
    "### CHOOSE CATCHMENTS\n",
    "###------------------------\n",
    "catchments_to_pull = np.arange(210, 260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40495d0-ead0-4500-b1f3-05137d38f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "###------------------------\n",
    "### CREATE FRAMEWORK\n",
    "###------------------------\n",
    "\n",
    "caps_and_disjoint = []\n",
    "\n",
    "## Perform Delaunay triangulation over each catchment region\n",
    "tri_ctmts = {i: [] for i in catchments_to_pull}\n",
    "for i in catchments_to_pull:\n",
    "    print('Triangulating catchment {}'.format(sf.record(i)['NAME']))\n",
    "    c = MultiPoint(sf.shape(i).points)\n",
    "    tris = triangulate(c)\n",
    "    tri_ctmts[i] = tris\n",
    "    if 'ICE_CAPS' in sf.record(i)['NAME']: ## catch the ice caps and weirdos after ID 200\n",
    "        caps_and_disjoint.append(i)\n",
    "    elif len(sf.shape(i).parts)>1:\n",
    "        caps_and_disjoint.append(i) \n",
    "\n",
    "# ## Perform Delaunay triangulation over each catchment region\n",
    "# tri_ctmts = {i: [] for i in catchments_to_pull}\n",
    "# for i in catchments_to_pull:\n",
    "#     print('Triangulating catchment {}'.format(sf.record(i)['NAME']))\n",
    "#     c = MultiPoint(sf.shape(i).points)\n",
    "#     tris = triangulate(c)\n",
    "#     tri_ctmts[i] = tris\n",
    "\n",
    "## Create data frames to store per-model time series\n",
    "model_names = ['ANICE-ITM_Berends', 'CESM_kampenhout', 'dEBM_krebs','HIRHAM_mottram', \n",
    "                'NHM-SMAP_niwano', 'RACMO_noel', 'SNOWMODEL_liston']\n",
    "# model_names = ['ANICE-ITM_Berends',]\n",
    "years = range(1980,2013)\n",
    "start_date = datetime.datetime(years[0],1,1)\n",
    "end_date = datetime.datetime(years[-1],12,31)\n",
    "dates = pd.date_range(start=start_date, end=end_date, freq='M')\n",
    "df_per_ctchmnt = {i: pd.DataFrame(columns=model_names, index=dates) for i in catchments_to_pull}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb0c93-9b4e-4fa7-965b-27b1112dbefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_and_disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8374e16-0362-4e64-827c-4dd463444a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###------------------------\n",
    "### CATCHMENT-SUM FOR ALL MODELS\n",
    "###------------------------\n",
    "\n",
    "for m in model_names:\n",
    "    t0 = time.time()\n",
    "    if m=='CESM_kampenhout':\n",
    "        vname = 'SMBCORR'\n",
    "    else:\n",
    "        vname = 'SMBcorr'\n",
    "    for y in years:\n",
    "        ti = time.time()\n",
    "        fpath = '/Volumes/GoogleDrive/My Drive/Greenland-data/SMBMIP/{}-monthly-ERA-Interim-{}.nc'.format(m, y)\n",
    "        attempts = 0 ## allow multiple attempts for connectivity issues\n",
    "        while attempts<=5:\n",
    "            try:\n",
    "                fh = Dataset(fpath, mode='r')\n",
    "                smb_m = fh.variables[vname][:].copy()\n",
    "                fh.close()\n",
    "                break\n",
    "            except RuntimeError:\n",
    "                print('RuntimeError encountered, waiting in case of connectivity issue...')\n",
    "                time.sleep(5) ## wait 5 seconds and try again\n",
    "                attempts+=1\n",
    "        d_subset = [d for d in dates if d.year==y]\n",
    "        for i in range(len(smb_m)): # for each month\n",
    "            ## downsample and interpolate SMB\n",
    "            smb_ds = smb_m[i][::2, ::2]\n",
    "            regridded_smb = interpolate.griddata((xs.ravel(), ys.ravel()), smb_ds.ravel(), (Xmat, Ymat), method='nearest')\n",
    "            ## Sample SMB at each Delaunay triangle and sum\n",
    "            for j in catchments_to_pull:\n",
    "                if j in caps_and_disjoint:\n",
    "#                 print('Identified catchment {} in ice caps and disjoint catchments. Skipping.'.format(j))\n",
    "#                 df_record.drop(columns=j, inplace=True)\n",
    "                    pass\n",
    "                else:\n",
    "                    catchment_sum = 0\n",
    "                    area_sum = 0\n",
    "                    triangles = tri_ctmts[j]\n",
    "                    for tri in triangles:\n",
    "                        rep_x, rep_y = tri.representative_point().x, tri.representative_point().y\n",
    "                        area_m2 = tri.area\n",
    "                        smb_x = (np.abs(x_3km - rep_x)).argmin()\n",
    "                        smb_y = (np.abs(y_3km - rep_y)).argmin()\n",
    "                        nearest_smb = regridded_smb[smb_y, smb_x]\n",
    "                        if nearest_smb < -6000: ## catch fill values or unrealistic extreme values\n",
    "                            next_smb_x = np.argsort(np.abs(x_3km-rep_x))[1]\n",
    "                            next_smb_y = np.argsort(np.abs(y_3km-rep_y))[1]\n",
    "                            next_nearest_smb = regridded_smb[next_smb_y, next_smb_x]\n",
    "                            ### TODO: add an option for a fill value, if next nearest is also bad?\n",
    "                            local_val = next_nearest_smb*area_m2\n",
    "                        elif nearest_smb > 6000: ## HIRHAM and SNOWMODEL have positive fill val\n",
    "                            next_smb_x = np.argsort(np.abs(x_3km-rep_x))[1]\n",
    "                            next_smb_y = np.argsort(np.abs(y_3km-rep_y))[1]\n",
    "                            next_nearest_smb = regridded_smb[next_smb_y, next_smb_x]\n",
    "                            ### TODO: add an option for a fill value, if next nearest is also bad?\n",
    "                            local_val = next_nearest_smb*area_m2\n",
    "                        else:\n",
    "                            local_val = nearest_smb*area_m2\n",
    "                        catchment_sum += local_val\n",
    "                        area_sum += area_m2\n",
    "                    df_per_ctchmnt[j][m][d_subset[i]] = catchment_sum/area_sum\n",
    "        tf = time.time()\n",
    "        print('Finished processing year {} in time {}s'.format(y, tf-ti))\n",
    "    t1 = time.time()\n",
    "    print('Finished processing model {} in time {}'.format(m, t1-t0))\n",
    "\n",
    "## Write to CSV\n",
    "for i in catchments_to_pull:  \n",
    "    csv_ctchmnt = '/Users/lizz/Documents/GitHub/Data_unsynced/SMBMIP-processed/{}-catchment_{}_mean-tseries.csv'.format(datetime.date.today().strftime('%Y%m%d'), i)\n",
    "    df_per_ctchmnt[i].to_csv(csv_ctchmnt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd2594d-3c61-4b92-845e-ad1f0d5db6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
